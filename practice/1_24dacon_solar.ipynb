{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.24dacon_solar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZNXKfRYSfb6UvOoce58EO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsja22/ai/blob/master/1_24dacon_solar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2ijDYimTAXU"
      },
      "source": [
        "#1095일의 데이터를 같은시간끼리 묶어서 해보기!\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import sys\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from tensorflow.keras.models import Sequential,Model ,load_model\r\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout,Lambda,MaxPooling2D, Conv2D, Flatten, Reshape, Conv1D, MaxPooling1D, Input,LeakyReLU\r\n",
        "from sklearn.metrics import mean_squared_error,r2_score\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n",
        "from tensorflow.keras.losses import Huber\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "\r\n",
        "def quantile_loss(q, y_true, y_pred):\r\n",
        "    err = (y_true - y_pred)\r\n",
        "    return K.mean(K.maximum(q*err, (q-1)*err), axis=-1)\r\n",
        "\r\n",
        "quantiles = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\r\n",
        "day = 7\r\n",
        "def preprocess_data(data,is_train=True):\r\n",
        "    data['cos'] = np.cos(np.pi/2 - np.abs(data['Hour']%12 - 6)/6*np.pi/2)\r\n",
        "    data.insert(1,'GHI',data['DNI']*data['cos']+data['DHI'])\r\n",
        "    temp = data.copy()\r\n",
        "    temp = temp[['DHI','DNI','GHI','T','WS','RH','TARGET']]\r\n",
        "    \r\n",
        "    if is_train == True:\r\n",
        "        temp['TARGET1'] = temp['TARGET'].shift(-48).fillna(method = 'ffill')\r\n",
        "        temp['TARGET2'] = temp['TARGET'].shift(-96).fillna(method = 'ffill')\r\n",
        "        temp = temp.dropna()\r\n",
        "        return temp.iloc[:-96]\r\n",
        "\r\n",
        "    elif is_train == False:\r\n",
        "        return temp.iloc[-48*day:, :]\r\n",
        "\r\n",
        "def split_xy(data,timestep):\r\n",
        "    x, y1, y2 = [],[],[]\r\n",
        "    for i in range(len(data)):\r\n",
        "        x_end = i + timestep\r\n",
        "        if x_end>len(data):\r\n",
        "            break\r\n",
        "        tmp_x = data[i:x_end]\r\n",
        "        tmp_y1 = data[x_end-1:x_end,-2]\r\n",
        "        tmp_y2 = data[x_end-1:x_end,-1]\r\n",
        "        x.append(tmp_x)\r\n",
        "        y1.append(tmp_y1)\r\n",
        "        y2.append(tmp_y2)\r\n",
        "    return(np.array(x),np.array(y1),np.array(y2))\r\n",
        "\r\n",
        "def Conv1dmodel():\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Conv1D(256,2,padding = 'same', activation = 'relu',input_shape = (7,7)))\r\n",
        "    model.add(Conv1D(128,2,padding = 'same', activation = 'relu'))\r\n",
        "    model.add(Conv1D(64,2,padding = 'same', activation = 'relu'))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(64, activation = 'relu'))\r\n",
        "    model.add(Dense(32, activation = 'relu'))\r\n",
        "    model.add(Dense(16, activation = 'relu'))\r\n",
        "    model.add(Dense(8, activation = 'relu'))\r\n",
        "    model.add(Dense(1))\r\n",
        "    return model\r\n",
        "\r\n",
        "#train 데이터 준비 \r\n",
        "train= pd.read_csv('C:/data/csv/solar/train/train.csv',index_col=None, header=0)\r\n",
        "print(train .shape)     #(52560, 9)\r\n",
        "print(train .tail())\r\n",
        "submission = pd.read_csv('C:/data/csv/solar/sample_submission.csv')\r\n",
        "\r\n",
        "df_train = preprocess_data(train,is_train=True)\r\n",
        "print(df_train.columns) #Index(['DHI', 'DNI', 'GHI', 'T', 'WS', 'RH', 'TARGET', 'TARGET1', 'TARGET2'], dtype='object')\r\n",
        "\r\n",
        "x_train = df_train.iloc[:,:-2]\r\n",
        "\r\n",
        "x_train = x_train.values\r\n",
        "\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "scale = StandardScaler()\r\n",
        "scale.fit(x_train)\r\n",
        "x_train = scale.transform(x_train)\r\n",
        "print(x_train.shape) #(52464, 7)\r\n",
        "\r\n",
        "x_train = x_train.reshape(int(x_train.shape[0]/48),48,7)\r\n",
        "print(x_train.shape)\r\n",
        "x_train = np.transpose(x_train, axes=(1,0,2))\r\n",
        "print(x_train.shape) #(48, 1093, 7)\r\n",
        "\r\n",
        "\r\n",
        "'''\r\n",
        "x_train = x_train.reshape(int(x_train.shape[0]/48),48,7)\r\n",
        "print(x_train.shape) #(1095, 48, 7)\r\n",
        "\r\n",
        "#1095일의 시간별 데이터를 1095일 단위로 묶어서 사용 하기위해 reshape해줘야함\r\n",
        "#(48,1095,7)로 변환\r\n",
        "\r\n",
        "x_train = np.transpose(x_train, axes=(1,0,2))\r\n",
        "print(x_train.shape) #(48, 1095, 7)\r\n",
        "x_train = x_train.reshape(x_train.shape[0]*x_train.shape[1],x_train.shape[2]) #(52560,7)\r\n",
        "\r\n",
        "'''\r\n",
        "# test data 준비\r\n",
        "\r\n",
        "x_test = []\r\n",
        "\r\n",
        "for i in range(81):\r\n",
        "    file_path = 'C:/data/csv/solar/test/' + str(i) + '.csv'\r\n",
        "    temp = pd.read_csv(file_path)\r\n",
        "    temp = preprocess_data(temp,is_train=False)\r\n",
        "    temp = scale.transform(temp)\r\n",
        "    temp = pd.DataFrame(temp)\r\n",
        "    x_test.append(temp)\r\n",
        "\r\n",
        "x_test = pd.concat(x_test)\r\n",
        "print(x_test.shape) #(27216, 7)\r\n",
        "x_test = x_test.values\r\n",
        "x_test = x_test.reshape(81,7,48,7)\r\n",
        "x_test = np.transpose(x_test, axes=(0,2,1,3))\r\n",
        "print(x_train.shape) #(48, 1093, 7)\r\n",
        "print(x_test.shape) #(81, 48, 7, 7) \r\n",
        "\r\n",
        "\r\n",
        "x,y1,y2 = [],[],[]\r\n",
        "for i in range(48):\r\n",
        "    tmp1,tmp2,tmp3 = split_xy(x_train[i],day)\r\n",
        "    x.append(tmp1)\r\n",
        "    y1.append(tmp2)\r\n",
        "    y2.append(tmp3)\r\n",
        "\r\n",
        "x = np.array(x) \r\n",
        "y1 = np.array(y1) \r\n",
        "y2 = np.array(y2) \r\n",
        "print(x.shape)\r\n",
        "\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\r\n",
        "es = EarlyStopping(monitor = 'val_loss', patience = 20)\r\n",
        "lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 8, factor = 0.3, verbose = 1)\r\n",
        "epochs = 500\r\n",
        "bs = 32\r\n",
        "\r\n",
        "for i in range(48):\r\n",
        "    x1_train, x1_val, y1_train, y1_val, y2_train, y2_val = train_test_split(x[i],y1[i],y2[i], train_size = 0.7,shuffle = True, random_state = 0)\r\n",
        "    print(x1_val.shape) #(327, 7, 7)\r\n",
        "    print(y1_val.shape) #(327, 1)\r\n",
        "    print(y2_val.shape) #(327, 1)\r\n",
        "    \r\n",
        "    hour = 0\r\n",
        "    \r\n",
        "\r\n",
        "    if i%2 == 0:\r\n",
        "      minute = 0\r\n",
        "    elif i%2 == 1:\r\n",
        "      minute = 30\r\n",
        "    hour += hour+int(i/2) \r\n",
        "\r\n",
        "    for j in quantiles:\r\n",
        "        print(\"##############내일 {}시,{}분, q_0.{} 훈련 시작!!###########\".format(minute,hour,j))\r\n",
        "        model = Conv1dmodel()\r\n",
        "        filepath_cp = f'C:/data/modelcheckpoint/solar_checkpoint_0124_{i:2d}_day1_{j:.1f}.hdf5'\r\n",
        "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\r\n",
        "        model.compile(loss = lambda y_true,y_pred: quantile_loss(j,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(j,y,y_pred)])\r\n",
        "        model.fit(x1_train,y1_train,epochs = epochs, batch_size = bs, validation_data = (x1_val,y1_val),callbacks = [es,cp,lr])\r\n",
        "\r\n",
        "    \r\n",
        "    for j in quantiles:\r\n",
        "        print(\"##############모레 {}시,{}분 q_0.{} 훈련 시작!!############\".format(minute,hour,j))\r\n",
        "        model = Conv1dmodel()\r\n",
        "        filepath_cp = f'C:/data/modelcheckpoint/solar_checkpoint_0124_{i:2d}_day2_{j:.1f}.hdf5'\r\n",
        "        cp = ModelCheckpoint(filepath_cp,save_best_only=True,monitor = 'val_loss')\r\n",
        "        model.compile(loss = lambda y_true,y_pred: quantile_loss(j,y_true,y_pred), optimizer = 'adam', metrics = [lambda y,y_pred: quantile_loss(j,y,y_pred)])\r\n",
        "        model.fit(x1_train,y2_train,epochs = epochs, batch_size = bs, validation_data = (x1_val,y2_val),callbacks = [es,cp,lr]) \r\n",
        "\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "#test 도 train과 같이 shape를 (48시간,81일,7일씩데이터,7컬럼) 으로 맞춰줘야함\r\n",
        "\r\n",
        "x_test = x_test.values\r\n",
        "x_test = x_test.reshape(81,7,48,7)\r\n",
        "x_test = np.transpose(x_test, axes=(2,0,1,3))\r\n",
        "print(x_test.shape)  #(48, 81, 7, 7)\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
